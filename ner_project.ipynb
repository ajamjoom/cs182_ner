{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# CS182 Final Project\n",
    "\n",
    "### Named-Entity Recognition using an HMM Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I think that building the tagger will have a large enough scope, especially when you show different approaches/ideas and try to maximize your scores. I am also happy to discuss in person early next week. \n",
    "\n",
    "Regarding your ideas: I am not sure why you want a full Bayes-Net - I believe a HMM+Viterbi is sufficient for the task so I recommend implementing that. Here are slides from CS287 that talk about NER and how to approach the problem: https://cs287.github.io/Lectures/slides/lecture14-search.pdf\n",
    "It could be useful for you to compare the performance between the forward and viterbi algorithm here. \n",
    "\n",
    "When you consider more information, please be wary that HMMs/Bayes-Nets have the Markov assumption. You need to be careful when taking previous and following words into account that you don’t violate that assumption. Depending on the size of your corpus, cross-validation might also not be necessary and a simple train/valid/test split could be sufficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_recall_fscore_support, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>lemma</th>\n",
       "      <th>next-lemma</th>\n",
       "      <th>next-next-lemma</th>\n",
       "      <th>next-next-pos</th>\n",
       "      <th>next-next-shape</th>\n",
       "      <th>next-next-word</th>\n",
       "      <th>next-pos</th>\n",
       "      <th>next-shape</th>\n",
       "      <th>next-word</th>\n",
       "      <th>...</th>\n",
       "      <th>prev-prev-lemma</th>\n",
       "      <th>prev-prev-pos</th>\n",
       "      <th>prev-prev-shape</th>\n",
       "      <th>prev-prev-word</th>\n",
       "      <th>prev-shape</th>\n",
       "      <th>prev-word</th>\n",
       "      <th>sentence_idx</th>\n",
       "      <th>shape</th>\n",
       "      <th>word</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>thousand</td>\n",
       "      <td>of</td>\n",
       "      <td>demonstr</td>\n",
       "      <td>NNS</td>\n",
       "      <td>lowercase</td>\n",
       "      <td>demonstrators</td>\n",
       "      <td>IN</td>\n",
       "      <td>lowercase</td>\n",
       "      <td>of</td>\n",
       "      <td>...</td>\n",
       "      <td>__start2__</td>\n",
       "      <td>__START2__</td>\n",
       "      <td>wildcard</td>\n",
       "      <td>__START2__</td>\n",
       "      <td>wildcard</td>\n",
       "      <td>__START1__</td>\n",
       "      <td>1.0</td>\n",
       "      <td>capitalized</td>\n",
       "      <td>Thousands</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>of</td>\n",
       "      <td>demonstr</td>\n",
       "      <td>have</td>\n",
       "      <td>VBP</td>\n",
       "      <td>lowercase</td>\n",
       "      <td>have</td>\n",
       "      <td>NNS</td>\n",
       "      <td>lowercase</td>\n",
       "      <td>demonstrators</td>\n",
       "      <td>...</td>\n",
       "      <td>__start1__</td>\n",
       "      <td>__START1__</td>\n",
       "      <td>wildcard</td>\n",
       "      <td>__START1__</td>\n",
       "      <td>capitalized</td>\n",
       "      <td>Thousands</td>\n",
       "      <td>1.0</td>\n",
       "      <td>lowercase</td>\n",
       "      <td>of</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>demonstr</td>\n",
       "      <td>have</td>\n",
       "      <td>march</td>\n",
       "      <td>VBN</td>\n",
       "      <td>lowercase</td>\n",
       "      <td>marched</td>\n",
       "      <td>VBP</td>\n",
       "      <td>lowercase</td>\n",
       "      <td>have</td>\n",
       "      <td>...</td>\n",
       "      <td>thousand</td>\n",
       "      <td>NNS</td>\n",
       "      <td>capitalized</td>\n",
       "      <td>Thousands</td>\n",
       "      <td>lowercase</td>\n",
       "      <td>of</td>\n",
       "      <td>1.0</td>\n",
       "      <td>lowercase</td>\n",
       "      <td>demonstrators</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>have</td>\n",
       "      <td>march</td>\n",
       "      <td>through</td>\n",
       "      <td>IN</td>\n",
       "      <td>lowercase</td>\n",
       "      <td>through</td>\n",
       "      <td>VBN</td>\n",
       "      <td>lowercase</td>\n",
       "      <td>marched</td>\n",
       "      <td>...</td>\n",
       "      <td>of</td>\n",
       "      <td>IN</td>\n",
       "      <td>lowercase</td>\n",
       "      <td>of</td>\n",
       "      <td>lowercase</td>\n",
       "      <td>demonstrators</td>\n",
       "      <td>1.0</td>\n",
       "      <td>lowercase</td>\n",
       "      <td>have</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>march</td>\n",
       "      <td>through</td>\n",
       "      <td>london</td>\n",
       "      <td>NNP</td>\n",
       "      <td>capitalized</td>\n",
       "      <td>London</td>\n",
       "      <td>IN</td>\n",
       "      <td>lowercase</td>\n",
       "      <td>through</td>\n",
       "      <td>...</td>\n",
       "      <td>demonstr</td>\n",
       "      <td>NNS</td>\n",
       "      <td>lowercase</td>\n",
       "      <td>demonstrators</td>\n",
       "      <td>lowercase</td>\n",
       "      <td>have</td>\n",
       "      <td>1.0</td>\n",
       "      <td>lowercase</td>\n",
       "      <td>marched</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0     lemma next-lemma next-next-lemma next-next-pos  \\\n",
       "0           0  thousand         of        demonstr           NNS   \n",
       "1           1        of   demonstr            have           VBP   \n",
       "2           2  demonstr       have           march           VBN   \n",
       "3           3      have      march         through            IN   \n",
       "4           4     march    through          london           NNP   \n",
       "\n",
       "  next-next-shape next-next-word next-pos next-shape      next-word ...  \\\n",
       "0       lowercase  demonstrators       IN  lowercase             of ...   \n",
       "1       lowercase           have      NNS  lowercase  demonstrators ...   \n",
       "2       lowercase        marched      VBP  lowercase           have ...   \n",
       "3       lowercase        through      VBN  lowercase        marched ...   \n",
       "4     capitalized         London       IN  lowercase        through ...   \n",
       "\n",
       "  prev-prev-lemma prev-prev-pos prev-prev-shape prev-prev-word   prev-shape  \\\n",
       "0      __start2__    __START2__        wildcard     __START2__     wildcard   \n",
       "1      __start1__    __START1__        wildcard     __START1__  capitalized   \n",
       "2        thousand           NNS     capitalized      Thousands    lowercase   \n",
       "3              of            IN       lowercase             of    lowercase   \n",
       "4        demonstr           NNS       lowercase  demonstrators    lowercase   \n",
       "\n",
       "       prev-word sentence_idx        shape           word tag  \n",
       "0     __START1__          1.0  capitalized      Thousands   O  \n",
       "1      Thousands          1.0    lowercase             of   O  \n",
       "2             of          1.0    lowercase  demonstrators   O  \n",
       "3  demonstrators          1.0    lowercase           have   O  \n",
       "4           have          1.0    lowercase        marched   O  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"entity-annotated-corpus/ner.csv\", encoding = \"ISO-8859-1\", error_bad_lines=False)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# drop null rows and check if any null values remaining\n",
    "data.dropna(inplace=True)\n",
    "data[data.isnull().any(axis=1)].size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Possible predictive data: [u'pos', u'shape', u'tag', u'prev-lemma', u'prev-prev-lemma', u'next-word', u'next-lemma', 'Unnamed: 0', u'next-next-word', u'lemma', u'prev-prev-iob', u'sentence_idx', u'next-next-pos', u'prev-iob', u'next-shape', u'prev-shape', u'prev-prev-word', u'next-next-shape', u'next-pos', u'prev-prev-pos', u'word', u'prev-pos', u'prev-prev-shape', u'prev-word', u'next-next-lemma']\n",
      "\n",
      "What do all of these mean?\n",
      "\n",
      "Tags: [u'I-art', u'B-gpe', u'B-art', u'I-per', u'I-tim', u'B-org', u'O', u'B-geo', u'B-tim', u'I-geo', u'B-per', u'I-eve', u'B-eve', u'I-gpe', u'I-org', u'I-nat', u'B-nat']\n",
      "\n",
      "What do all of these mean?\n",
      "TEST\n",
      "\n",
      "Length of data set: 1048573\n"
     ]
    }
   ],
   "source": [
    "# SOME EDA\n",
    "print(\"Possible predictive data: \" + str(list(set(data.columns.values))))\n",
    "print\n",
    "print(\"What do all of these mean?\")\n",
    "print\n",
    "print(\"Tags: \" + str(list(set(data.tag))))\n",
    "print\n",
    "print(\"What do all of these mean?\")\n",
    "print(\"TEST\")\n",
    "print\n",
    "print(\"Length of data set: \" + str(len(data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16000, 24)\n",
      "(16000,)\n"
     ]
    }
   ],
   "source": [
    "#data_small = data[:100000]\n",
    "#data_valid = data[100001:150000]\n",
    "\n",
    "data_small = data[:20000]\n",
    "data_valid = data[20001:30000]\n",
    "\n",
    "preds = list(data.columns.values)\n",
    "preds.remove('tag')\n",
    "y_small = data_small['tag']\n",
    "x_small = data_small[preds]\n",
    "\n",
    "# Split into train and test data\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_small, y_small, test_size=0.2, random_state=0)\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos_list\n",
      "\n",
      "[u'PRP$', u'VBG', u'VBD', u'``', u'VBN', u'POS', u'VBP', u'WDT', u'JJ', u'WP', u'VBZ', u'DT', u'RP', u'$', u'NN', u',', u'.', u'TO', u'PRP', u'RB', u';', u':', u'NNS', u'NNP', u'VB', u'WRB', u'RRB', u'CC', u'PDT', u'RBS', u'RBR', u'CD', u'NNPS', u'EX', u'IN', u'WP$', u'MD', u'LRB', u'JJS', u'JJR']\n",
      "\n",
      "shape_list\n",
      "\n",
      "[u'mixedcase', u'lowercase', u'camelcase', u'uppercase', u'capitalized', u'number', u'abbreviation', u'punct', u'other', u'ending-dot', u'contains-hyphen']\n",
      "\n",
      "tag_list:\n",
      "\n",
      "[u'I-art', u'B-gpe', u'B-art', u'I-per', u'I-tim', u'B-org', u'O', u'B-geo', u'B-tim', u'I-geo', u'B-per', u'I-eve', u'B-eve', u'I-gpe', u'I-org', u'I-nat', u'B-nat']\n"
     ]
    }
   ],
   "source": [
    "pos_list = list(set(x_train['pos']))\n",
    "print(\"pos_list\")\n",
    "print\n",
    "print(pos_list)\n",
    "print\n",
    "shape_list = list(set(x_train['shape']))\n",
    "print(\"shape_list\")\n",
    "print\n",
    "print(shape_list)\n",
    "print\n",
    "tag_list = list(set(y_train.values))\n",
    "print(\"tag_list:\")\n",
    "print\n",
    "print(tag_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'I-art', u'B-gpe', u'B-art', u'I-per', u'I-tim', u'B-org', u'O', u'B-geo', u'B-tim', u'I-geo', u'B-per', u'I-eve', u'B-eve', u'I-gpe', u'I-org', u'I-nat', u'B-nat']\n",
      "\n",
      "{u'mixedcase': u'B-art', u'lowercase': u'O', u'camelcase': u'B-org', u'uppercase': u'O', u'number': u'O', u'capitalized': u'O', u'abbreviation': u'B-gpe', u'punct': u'O', u'other': u'O', u'ending-dot': u'B-per', u'contains-hyphen': u'O'}\n",
      "\n",
      "{u'PRP$': u'O', u'VBG': u'O', u'VBD': u'O', u'VBN': u'O', u',': u'O', u'VBP': u'O', u'WDT': u'O', u'JJ': u'O', u'WP': u'O', u'VBZ': u'O', u'DT': u'O', u'RP': u'O', u'$': u'O', u'NN': u'O', u'POS': u'O', u'.': u'O', u'TO': u'O', u'PRP': u'O', u'RB': u'O', u';': u'O', u':': u'O', u'NNS': u'O', u'NNP': u'B-geo', u'``': u'O', u'WRB': u'O', u'RRB': u'O', u'CC': u'O', u'PDT': u'O', u'RBS': u'O', u'RBR': u'O', u'CD': u'O', u'LRB': u'O', u'EX': u'O', u'IN': u'O', u'WP$': u'O', u'MD': u'O', u'NNPS': u'I-org', u'JJS': u'O', u'JJR': u'O', u'VB': u'O'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# build a dict linking shape to likelihood of each tag\n",
    "\n",
    "shape_probs = {}\n",
    "pos_probs = {}\n",
    "\n",
    "# if X shape, then what is the most likely tag?\n",
    "\n",
    "print(tag_list)\n",
    "print\n",
    "\n",
    "for shape in shape_list:\n",
    "    tag_prob_list = []\n",
    "    for tag in tag_list:\n",
    "        count = 0\n",
    "        for i in data_small[data_small['shape'] == shape]['tag']:\n",
    "            if i == tag:\n",
    "                count += 1\n",
    "        tag_prob_list.append(1.0*count/(len(data_small[data_small['shape'] == shape]) + 1.0))\n",
    "    index = tag_prob_list.index(max(tag_prob_list))\n",
    "        \n",
    "    shape_probs[shape] = tag_list[index]\n",
    "    \n",
    "for pos in pos_list:\n",
    "    tag_prob_list = []\n",
    "    for tag in tag_list:\n",
    "        count = 0\n",
    "        for i in data_small[data_small['pos'] == pos]['tag']:\n",
    "            if i == tag:\n",
    "                count += 1\n",
    "        tag_prob_list.append(1.0*count/(len(data_small[data_small['shape'] == pos])  + 1.0))\n",
    "    index = tag_prob_list.index(max(tag_prob_list))\n",
    "        \n",
    "    pos_probs[pos] = tag_list[index]\n",
    "\n",
    "print(shape_probs)\n",
    "print\n",
    "print(pos_probs)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percent of data without a tag (Baseline accuracy if we only predict 'O': 0.846933880617\n",
      "Train Accuracy using pos: 0.8799\n",
      "Validation Accuracy using pos: 0.873087308731\n"
     ]
    }
   ],
   "source": [
    "pred_train = []\n",
    "pred_valid = []\n",
    "\n",
    "# make predictions based off of \"shape\"\n",
    "\n",
    "num_O = len(data[data['tag'] == 'O'])\n",
    "percent = 1.0*num_O/len(data)\n",
    "print \"Percent of data without a tag (Baseline accuracy if we only predict 'O': \" + str(percent)\n",
    "\n",
    "# training prediction\n",
    "count_correct = 0\n",
    "for i in range(len(data_small)):\n",
    "    try:\n",
    "        pred_tag = pos_probs[data_small.iloc[i]['pos']]\n",
    "    except:\n",
    "        do='nothing' # figure out this case later\n",
    "        #print(data_small.iloc[i]['pos'])\n",
    "    pred_train.append(pred_tag)\n",
    "    if data_small.iloc[i]['tag'] == pred_tag:\n",
    "        count_correct += 1\n",
    "train_accuracy = 1.0*count_correct / len(data_small)\n",
    "print \"Train Accuracy using pos: \" + str(train_accuracy)\n",
    "\n",
    "# validation prediction\n",
    "count_correct = 0\n",
    "for i in range(len(data_valid)):\n",
    "    try:\n",
    "        pred_tag = pos_probs[data_valid.iloc[i]['pos']]\n",
    "    except:\n",
    "        do='nothing' # figure out this case later\n",
    "        #print(data_valid.iloc[i]['pos'])\n",
    "    pred_train.append(pred_tag)\n",
    "    if data_valid.iloc[i]['tag'] == pred_tag:\n",
    "        count_correct += 1\n",
    "train_accuracy = 1.0*count_correct / len(data_valid)\n",
    "print \"Validation Accuracy using pos: \" + str(train_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### These are good prelim models, but we need to store probabilities differently to follow the Viterbi algorithm and incorporate multiple aspects of our dataset into the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'I-art', u'B-gpe', u'B-art', u'I-per', u'I-tim', u'B-org', u'O', u'B-geo', u'B-tim', u'I-geo', u'B-per', u'I-eve', u'B-eve', u'I-gpe', u'I-org', u'I-nat', u'B-nat']\n",
      "\n",
      "{u'mixedcase': {'I-art': 0.08333333333333333, 'B-nat': 0.08333333333333333, 'B-gpe': 0.08333333333333333, 'B-art': 0.16666666666666666, 'I-tim': 0.08333333333333333, 'B-org': 0.08333333333333333, 'O': 0.08333333333333333, 'B-geo': 0.08333333333333333, 'I-org': 0.08333333333333333, 'I-geo': 0.08333333333333333, 'I-per': 0.08333333333333333, 'I-eve': 0.08333333333333333, 'B-eve': 0.08333333333333333, 'I-gpe': 0.08333333333333333, 'B-tim': 0.08333333333333333, 'I-nat': 0.08333333333333333, 'B-per': 0.08333333333333333}, u'lowercase': {'I-art': 0.0001397233477714126, 'B-nat': 6.98616738857063e-05, 'B-gpe': 6.98616738857063e-05, 'B-art': 0.0001397233477714126, 'I-tim': 0.0010479251082855946, 'B-org': 0.0002095850216571189, 'O': 0.9942713427413721, 'B-geo': 0.0002794466955428252, 'I-org': 0.0016766801732569512, 'I-geo': 0.0002095850216571189, 'I-per': 0.0003493083694285315, 'I-eve': 6.98616738857063e-05, 'B-eve': 6.98616738857063e-05, 'I-gpe': 0.0002095850216571189, 'B-tim': 0.0014670951515998324, 'I-nat': 6.98616738857063e-05, 'B-per': 6.98616738857063e-05}, u'camelcase': {'I-art': 0.058823529411764705, 'B-nat': 0.058823529411764705, 'B-gpe': 0.058823529411764705, 'B-art': 0.058823529411764705, 'I-tim': 0.058823529411764705, 'B-org': 0.29411764705882354, 'O': 0.058823529411764705, 'B-geo': 0.058823529411764705, 'I-org': 0.058823529411764705, 'I-geo': 0.058823529411764705, 'I-per': 0.11764705882352941, 'I-eve': 0.058823529411764705, 'B-eve': 0.058823529411764705, 'I-gpe': 0.058823529411764705, 'B-tim': 0.058823529411764705, 'I-nat': 0.058823529411764705, 'B-per': 0.11764705882352941}, u'uppercase': {'I-art': 0.008, 'B-nat': 0.032, 'B-gpe': 0.032, 'B-art': 0.008, 'I-tim': 0.016, 'B-org': 0.352, 'O': 0.472, 'B-geo': 0.008, 'I-org': 0.016, 'I-geo': 0.008, 'I-per': 0.024, 'I-eve': 0.024, 'B-eve': 0.008, 'I-gpe': 0.008, 'B-tim': 0.008, 'I-nat': 0.008, 'B-per': 0.016}, u'number': {'I-art': 0.00684931506849315, 'B-nat': 0.003424657534246575, 'B-gpe': 0.003424657534246575, 'B-art': 0.00684931506849315, 'I-tim': 0.08904109589041095, 'B-org': 0.003424657534246575, 'O': 0.6404109589041096, 'B-geo': 0.003424657534246575, 'I-org': 0.003424657534246575, 'I-geo': 0.003424657534246575, 'I-per': 0.003424657534246575, 'I-eve': 0.003424657534246575, 'B-eve': 0.010273972602739725, 'I-gpe': 0.003424657534246575, 'B-tim': 0.22945205479452055, 'I-nat': 0.003424657534246575, 'B-per': 0.003424657534246575}, u'capitalized': {'I-art': 0.006027918781725888, 'B-nat': 0.0012690355329949238, 'B-gpe': 0.1434010152284264, 'B-art': 0.01015228426395939, 'I-tim': 0.0028553299492385786, 'B-org': 0.08565989847715735, 'O': 0.2223984771573604, 'B-geo': 0.15418781725888325, 'I-org': 0.078997461928934, 'I-geo': 0.024428934010152285, 'I-per': 0.10786802030456853, 'I-eve': 0.004124365482233503, 'B-eve': 0.005393401015228426, 'I-gpe': 0.007931472081218274, 'B-tim': 0.07106598984771574, 'I-nat': 0.0019035532994923859, 'B-per': 0.07423857868020305}, u'abbreviation': {'I-art': 0.008620689655172414, 'B-nat': 0.008620689655172414, 'B-gpe': 0.4396551724137931, 'B-art': 0.008620689655172414, 'I-tim': 0.008620689655172414, 'B-org': 0.31896551724137934, 'O': 0.008620689655172414, 'B-geo': 0.15517241379310345, 'I-org': 0.008620689655172414, 'I-geo': 0.017241379310344827, 'I-per': 0.008620689655172414, 'I-eve': 0.008620689655172414, 'B-eve': 0.008620689655172414, 'I-gpe': 0.008620689655172414, 'B-tim': 0.017241379310344827, 'I-nat': 0.008620689655172414, 'B-per': 0.008620689655172414}, u'punct': {'I-art': 0.0006042296072507553, 'B-nat': 0.0006042296072507553, 'B-gpe': 0.0006042296072507553, 'B-art': 0.0006042296072507553, 'I-tim': 0.0018126888217522659, 'B-org': 0.0006042296072507553, 'O': 0.9921450151057402, 'B-geo': 0.0006042296072507553, 'I-org': 0.0012084592145015106, 'I-geo': 0.0006042296072507553, 'I-per': 0.0006042296072507553, 'I-eve': 0.0006042296072507553, 'B-eve': 0.0006042296072507553, 'I-gpe': 0.0006042296072507553, 'B-tim': 0.0006042296072507553, 'I-nat': 0.0006042296072507553, 'B-per': 0.0006042296072507553}, u'other': {'I-art': 0.004524886877828055, 'B-nat': 0.01809954751131222, 'B-gpe': 0.00904977375565611, 'B-art': 0.004524886877828055, 'I-tim': 0.004524886877828055, 'B-org': 0.04072398190045249, 'O': 0.8733031674208145, 'B-geo': 0.013574660633484163, 'I-org': 0.01809954751131222, 'I-geo': 0.004524886877828055, 'I-per': 0.004524886877828055, 'I-eve': 0.004524886877828055, 'B-eve': 0.004524886877828055, 'I-gpe': 0.004524886877828055, 'B-tim': 0.00904977375565611, 'I-nat': 0.004524886877828055, 'B-per': 0.004524886877828055}, u'ending-dot': {'I-art': 0.017241379310344827, 'B-nat': 0.017241379310344827, 'B-gpe': 0.017241379310344827, 'B-art': 0.017241379310344827, 'I-tim': 0.017241379310344827, 'B-org': 0.017241379310344827, 'O': 0.034482758620689655, 'B-geo': 0.017241379310344827, 'I-org': 0.017241379310344827, 'I-geo': 0.017241379310344827, 'I-per': 0.017241379310344827, 'I-eve': 0.017241379310344827, 'B-eve': 0.017241379310344827, 'I-gpe': 0.017241379310344827, 'B-tim': 0.017241379310344827, 'I-nat': 0.017241379310344827, 'B-per': 0.8103448275862069}, u'contains-hyphen': {'I-art': 0.018867924528301886, 'B-nat': 0.006289308176100629, 'B-gpe': 0.018867924528301886, 'B-art': 0.025157232704402517, 'I-tim': 0.006289308176100629, 'B-org': 0.11320754716981132, 'O': 0.6540880503144654, 'B-geo': 0.031446540880503145, 'I-org': 0.025157232704402517, 'I-geo': 0.006289308176100629, 'I-per': 0.06289308176100629, 'I-eve': 0.006289308176100629, 'B-eve': 0.006289308176100629, 'I-gpe': 0.006289308176100629, 'B-tim': 0.012578616352201259, 'I-nat': 0.006289308176100629, 'B-per': 0.031446540880503145}}\n",
      "\n",
      "{u'PRP$': {'I-art': 0.025, 'B-nat': 0.025, 'B-gpe': 0.025, 'B-art': 0.025, 'I-tim': 0.05, 'B-org': 0.025, 'O': 3.75, 'B-geo': 0.025, 'I-org': 0.025, 'I-geo': 0.025, 'I-per': 0.025, 'I-eve': 0.025, 'B-eve': 0.025, 'I-gpe': 0.025, 'B-tim': 0.025, 'I-nat': 0.025, 'B-per': 0.025}, u'VBG': {'I-art': 0.025, 'B-nat': 0.025, 'B-gpe': 0.025, 'B-art': 0.025, 'I-tim': 0.025, 'B-org': 0.025, 'O': 9.075, 'B-geo': 0.025, 'I-org': 0.025, 'I-geo': 0.025, 'I-per': 0.025, 'I-eve': 0.025, 'B-eve': 0.025, 'I-gpe': 0.025, 'B-tim': 0.025, 'I-nat': 0.025, 'B-per': 0.025}, u'VBD': {'I-art': 0.025, 'B-nat': 0.025, 'B-gpe': 0.025, 'B-art': 0.025, 'I-tim': 0.025, 'B-org': 0.025, 'O': 18.4, 'B-geo': 0.025, 'I-org': 0.025, 'I-geo': 0.025, 'I-per': 0.025, 'I-eve': 0.025, 'B-eve': 0.025, 'I-gpe': 0.025, 'B-tim': 0.025, 'I-nat': 0.025, 'B-per': 0.025}, u'VBN': {'I-art': 0.025, 'B-nat': 0.025, 'B-gpe': 0.025, 'B-art': 0.025, 'I-tim': 0.025, 'B-org': 0.025, 'O': 15.875, 'B-geo': 0.025, 'I-org': 0.025, 'I-geo': 0.025, 'I-per': 0.025, 'I-eve': 0.025, 'B-eve': 0.025, 'I-gpe': 0.025, 'B-tim': 0.025, 'I-nat': 0.025, 'B-per': 0.025}, u',': {'I-art': 0.025, 'B-nat': 0.025, 'B-gpe': 0.025, 'B-art': 0.025, 'I-tim': 0.05, 'B-org': 0.025, 'O': 14.55, 'B-geo': 0.025, 'I-org': 0.025, 'I-geo': 0.025, 'I-per': 0.025, 'I-eve': 0.025, 'B-eve': 0.025, 'I-gpe': 0.025, 'B-tim': 0.025, 'I-nat': 0.025, 'B-per': 0.025}, u'VBP': {'I-art': 0.025, 'B-nat': 0.025, 'B-gpe': 0.025, 'B-art': 0.025, 'I-tim': 0.025, 'B-org': 0.025, 'O': 9.05, 'B-geo': 0.025, 'I-org': 0.025, 'I-geo': 0.025, 'I-per': 0.025, 'I-eve': 0.025, 'B-eve': 0.025, 'I-gpe': 0.025, 'B-tim': 0.025, 'I-nat': 0.025, 'B-per': 0.025}, u'WDT': {'I-art': 0.025, 'B-nat': 0.025, 'B-gpe': 0.025, 'B-art': 0.025, 'I-tim': 0.025, 'B-org': 0.025, 'O': 1.4, 'B-geo': 0.025, 'I-org': 0.025, 'I-geo': 0.025, 'I-per': 0.025, 'I-eve': 0.025, 'B-eve': 0.025, 'I-gpe': 0.025, 'B-tim': 0.025, 'I-nat': 0.025, 'B-per': 0.025}, u'JJ': {'I-art': 0.025, 'B-nat': 0.025, 'B-gpe': 6.8, 'B-art': 0.05, 'I-tim': 0.025, 'B-org': 0.2, 'O': 29.0, 'B-geo': 0.2, 'I-org': 0.025, 'I-geo': 0.025, 'I-per': 0.025, 'I-eve': 0.05, 'B-eve': 0.025, 'I-gpe': 0.1, 'B-tim': 0.25, 'I-nat': 0.025, 'B-per': 0.025}, u'WP': {'I-art': 0.025, 'B-nat': 0.025, 'B-gpe': 0.025, 'B-art': 0.025, 'I-tim': 0.025, 'B-org': 0.025, 'O': 1.025, 'B-geo': 0.025, 'I-org': 0.025, 'I-geo': 0.025, 'I-per': 0.025, 'I-eve': 0.025, 'B-eve': 0.025, 'I-gpe': 0.025, 'B-tim': 0.025, 'I-nat': 0.025, 'B-per': 0.025}, u'VBZ': {'I-art': 0.025, 'B-nat': 0.025, 'B-gpe': 0.025, 'B-art': 0.025, 'I-tim': 0.025, 'B-org': 0.025, 'O': 11.75, 'B-geo': 0.025, 'I-org': 0.025, 'I-geo': 0.025, 'I-per': 0.025, 'I-eve': 0.025, 'B-eve': 0.025, 'I-gpe': 0.025, 'B-tim': 0.025, 'I-nat': 0.025, 'B-per': 0.025}, u'DT': {'I-art': 0.025, 'B-nat': 0.025, 'B-gpe': 0.025, 'B-art': 0.05, 'I-tim': 0.025, 'B-org': 0.025, 'O': 48.45, 'B-geo': 0.025, 'I-org': 0.025, 'I-geo': 0.025, 'I-per': 0.025, 'I-eve': 0.025, 'B-eve': 0.025, 'I-gpe': 0.025, 'B-tim': 0.05, 'I-nat': 0.025, 'B-per': 0.025}, u'RP': {'I-art': 0.025, 'B-nat': 0.025, 'B-gpe': 0.025, 'B-art': 0.025, 'I-tim': 0.025, 'B-org': 0.025, 'O': 1.125, 'B-geo': 0.025, 'I-org': 0.025, 'I-geo': 0.025, 'I-per': 0.025, 'I-eve': 0.025, 'B-eve': 0.025, 'I-gpe': 0.025, 'B-tim': 0.025, 'I-nat': 0.025, 'B-per': 0.025}, u'$': {'I-art': 0.025, 'B-nat': 0.025, 'B-gpe': 0.025, 'B-art': 0.025, 'I-tim': 0.025, 'B-org': 0.025, 'O': 0.625, 'B-geo': 0.025, 'I-org': 0.025, 'I-geo': 0.025, 'I-per': 0.025, 'I-eve': 0.025, 'B-eve': 0.025, 'I-gpe': 0.025, 'B-tim': 0.025, 'I-nat': 0.025, 'B-per': 0.025}, u'NN': {'I-art': 0.025, 'B-nat': 0.025, 'B-gpe': 0.05, 'B-art': 0.05, 'I-tim': 0.2, 'B-org': 0.125, 'O': 67.9, 'B-geo': 0.05, 'I-org': 0.125, 'I-geo': 0.05, 'I-per': 0.025, 'I-eve': 0.025, 'B-eve': 0.025, 'I-gpe': 0.05, 'B-tim': 0.125, 'I-nat': 0.025, 'B-per': 0.025}, u'POS': {'I-art': 0.025, 'B-nat': 0.025, 'B-gpe': 0.025, 'B-art': 0.025, 'I-tim': 0.025, 'B-org': 0.025, 'O': 4.625, 'B-geo': 0.025, 'I-org': 0.125, 'I-geo': 0.025, 'I-per': 0.025, 'I-eve': 0.025, 'B-eve': 0.025, 'I-gpe': 0.025, 'B-tim': 0.05, 'I-nat': 0.025, 'B-per': 0.025}, u'.': {'I-art': 0.025, 'B-nat': 0.025, 'B-gpe': 0.025, 'B-art': 0.025, 'I-tim': 0.025, 'B-org': 0.025, 'O': 22.55, 'B-geo': 0.025, 'I-org': 0.025, 'I-geo': 0.025, 'I-per': 0.025, 'I-eve': 0.025, 'B-eve': 0.025, 'I-gpe': 0.025, 'B-tim': 0.025, 'I-nat': 0.025, 'B-per': 0.025}, u'TO': {'I-art': 0.025, 'B-nat': 0.025, 'B-gpe': 0.025, 'B-art': 0.025, 'I-tim': 0.075, 'B-org': 0.025, 'O': 11.05, 'B-geo': 0.025, 'I-org': 0.025, 'I-geo': 0.025, 'I-per': 0.025, 'I-eve': 0.025, 'B-eve': 0.025, 'I-gpe': 0.025, 'B-tim': 0.025, 'I-nat': 0.025, 'B-per': 0.025}, u'PRP': {'I-art': 0.025, 'B-nat': 0.025, 'B-gpe': 0.025, 'B-art': 0.025, 'I-tim': 0.025, 'B-org': 0.025, 'O': 8.025, 'B-geo': 0.025, 'I-org': 0.025, 'I-geo': 0.025, 'I-per': 0.025, 'I-eve': 0.025, 'B-eve': 0.025, 'I-gpe': 0.025, 'B-tim': 0.025, 'I-nat': 0.025, 'B-per': 0.025}, u'RB': {'I-art': 0.025, 'B-nat': 0.025, 'B-gpe': 0.025, 'B-art': 0.025, 'I-tim': 0.025, 'B-org': 0.025, 'O': 9.675, 'B-geo': 0.025, 'I-org': 0.025, 'I-geo': 0.025, 'I-per': 0.025, 'I-eve': 0.025, 'B-eve': 0.025, 'I-gpe': 0.025, 'B-tim': 0.075, 'I-nat': 0.025, 'B-per': 0.025}, u';': {'I-art': 0.025, 'B-nat': 0.025, 'B-gpe': 0.025, 'B-art': 0.025, 'I-tim': 0.025, 'B-org': 0.025, 'O': 0.075, 'B-geo': 0.025, 'I-org': 0.025, 'I-geo': 0.025, 'I-per': 0.025, 'I-eve': 0.025, 'B-eve': 0.025, 'I-gpe': 0.025, 'B-tim': 0.025, 'I-nat': 0.025, 'B-per': 0.025}, u':': {'I-art': 0.025, 'B-nat': 0.025, 'B-gpe': 0.025, 'B-art': 0.025, 'I-tim': 0.05, 'B-org': 0.025, 'O': 0.4, 'B-geo': 0.025, 'I-org': 0.025, 'I-geo': 0.025, 'I-per': 0.025, 'I-eve': 0.025, 'B-eve': 0.025, 'I-gpe': 0.025, 'B-tim': 0.025, 'I-nat': 0.025, 'B-per': 0.025}, u'NNS': {'I-art': 0.025, 'B-nat': 0.025, 'B-gpe': 0.625, 'B-art': 0.025, 'I-tim': 0.05, 'B-org': 0.125, 'O': 37.425, 'B-geo': 0.025, 'I-org': 0.075, 'I-geo': 0.05, 'I-per': 0.025, 'I-eve': 0.025, 'B-eve': 0.025, 'I-gpe': 0.025, 'B-tim': 0.2, 'I-nat': 0.025, 'B-per': 0.025}, u'NNP': {'I-art': 0.5, 'B-nat': 0.25, 'B-gpe': 5.3, 'B-art': 0.875, 'I-tim': 0.25, 'B-org': 9.075, 'O': 0.7, 'B-geo': 12.575, 'I-org': 5.95, 'I-geo': 1.75, 'I-per': 8.875, 'I-eve': 0.275, 'B-eve': 0.375, 'I-gpe': 0.425, 'B-tim': 5.575, 'I-nat': 0.15, 'B-per': 7.1}, u'``': {'I-art': 0.025, 'B-nat': 0.025, 'B-gpe': 0.025, 'B-art': 0.025, 'I-tim': 0.025, 'B-org': 0.025, 'O': 1.825, 'B-geo': 0.025, 'I-org': 0.025, 'I-geo': 0.025, 'I-per': 0.025, 'I-eve': 0.025, 'B-eve': 0.025, 'I-gpe': 0.025, 'B-tim': 0.025, 'I-nat': 0.025, 'B-per': 0.025}, u'WRB': {'I-art': 0.025, 'B-nat': 0.025, 'B-gpe': 0.025, 'B-art': 0.025, 'I-tim': 0.025, 'B-org': 0.025, 'O': 1.625, 'B-geo': 0.025, 'I-org': 0.025, 'I-geo': 0.025, 'I-per': 0.025, 'I-eve': 0.025, 'B-eve': 0.025, 'I-gpe': 0.025, 'B-tim': 0.025, 'I-nat': 0.025, 'B-per': 0.025}, u'RRB': {'I-art': 0.025, 'B-nat': 0.025, 'B-gpe': 0.025, 'B-art': 0.025, 'I-tim': 0.025, 'B-org': 0.025, 'O': 0.3, 'B-geo': 0.025, 'I-org': 0.025, 'I-geo': 0.025, 'I-per': 0.025, 'I-eve': 0.025, 'B-eve': 0.025, 'I-gpe': 0.025, 'B-tim': 0.025, 'I-nat': 0.025, 'B-per': 0.025}, u'CC': {'I-art': 0.025, 'B-nat': 0.025, 'B-gpe': 0.025, 'B-art': 0.025, 'I-tim': 0.025, 'B-org': 0.025, 'O': 10.2, 'B-geo': 0.025, 'I-org': 0.15, 'I-geo': 0.025, 'I-per': 0.025, 'I-eve': 0.025, 'B-eve': 0.025, 'I-gpe': 0.025, 'B-tim': 0.025, 'I-nat': 0.025, 'B-per': 0.025}, u'PDT': {'I-art': 0.025, 'B-nat': 0.025, 'B-gpe': 0.025, 'B-art': 0.025, 'I-tim': 0.025, 'B-org': 0.025, 'O': 0.1, 'B-geo': 0.025, 'I-org': 0.025, 'I-geo': 0.025, 'I-per': 0.025, 'I-eve': 0.025, 'B-eve': 0.025, 'I-gpe': 0.025, 'B-tim': 0.025, 'I-nat': 0.025, 'B-per': 0.025}, u'RBS': {'I-art': 0.025, 'B-nat': 0.025, 'B-gpe': 0.025, 'B-art': 0.025, 'I-tim': 0.025, 'B-org': 0.025, 'O': 0.175, 'B-geo': 0.025, 'I-org': 0.025, 'I-geo': 0.025, 'I-per': 0.025, 'I-eve': 0.025, 'B-eve': 0.025, 'I-gpe': 0.025, 'B-tim': 0.025, 'I-nat': 0.025, 'B-per': 0.025}, u'RBR': {'I-art': 0.025, 'B-nat': 0.025, 'B-gpe': 0.025, 'B-art': 0.025, 'I-tim': 0.025, 'B-org': 0.025, 'O': 0.575, 'B-geo': 0.025, 'I-org': 0.025, 'I-geo': 0.025, 'I-per': 0.025, 'I-eve': 0.025, 'B-eve': 0.025, 'I-gpe': 0.025, 'B-tim': 0.025, 'I-nat': 0.025, 'B-per': 0.025}, u'CD': {'I-art': 0.05, 'B-nat': 0.025, 'B-gpe': 0.025, 'B-art': 0.025, 'I-tim': 0.65, 'B-org': 0.025, 'O': 9.125, 'B-geo': 0.025, 'I-org': 0.025, 'I-geo': 0.025, 'I-per': 0.025, 'I-eve': 0.025, 'B-eve': 0.075, 'I-gpe': 0.025, 'B-tim': 1.575, 'I-nat': 0.025, 'B-per': 0.025}, u'LRB': {'I-art': 0.025, 'B-nat': 0.025, 'B-gpe': 0.025, 'B-art': 0.025, 'I-tim': 0.025, 'B-org': 0.025, 'O': 0.3, 'B-geo': 0.025, 'I-org': 0.025, 'I-geo': 0.025, 'I-per': 0.025, 'I-eve': 0.025, 'B-eve': 0.025, 'I-gpe': 0.025, 'B-tim': 0.025, 'I-nat': 0.025, 'B-per': 0.025}, u'EX': {'I-art': 0.025, 'B-nat': 0.025, 'B-gpe': 0.025, 'B-art': 0.025, 'I-tim': 0.025, 'B-org': 0.025, 'O': 0.425, 'B-geo': 0.025, 'I-org': 0.025, 'I-geo': 0.025, 'I-per': 0.025, 'I-eve': 0.025, 'B-eve': 0.025, 'I-gpe': 0.025, 'B-tim': 0.025, 'I-nat': 0.025, 'B-per': 0.025}, u'IN': {'I-art': 0.05, 'B-nat': 0.025, 'B-gpe': 0.025, 'B-art': 0.025, 'I-tim': 0.1, 'B-org': 0.025, 'O': 58.475, 'B-geo': 0.025, 'I-org': 0.4, 'I-geo': 0.05, 'I-per': 0.025, 'I-eve': 0.025, 'B-eve': 0.025, 'I-gpe': 0.075, 'B-tim': 0.125, 'I-nat': 0.025, 'B-per': 0.025}, u'WP$': {'I-art': 0.025, 'B-nat': 0.025, 'B-gpe': 0.025, 'B-art': 0.025, 'I-tim': 0.025, 'B-org': 0.025, 'O': 0.05, 'B-geo': 0.025, 'I-org': 0.025, 'I-geo': 0.025, 'I-per': 0.025, 'I-eve': 0.025, 'B-eve': 0.025, 'I-gpe': 0.025, 'B-tim': 0.025, 'I-nat': 0.025, 'B-per': 0.025}, u'MD': {'I-art': 0.025, 'B-nat': 0.025, 'B-gpe': 0.025, 'B-art': 0.025, 'I-tim': 0.025, 'B-org': 0.025, 'O': 3.725, 'B-geo': 0.025, 'I-org': 0.025, 'I-geo': 0.025, 'I-per': 0.025, 'I-eve': 0.025, 'B-eve': 0.025, 'I-gpe': 0.025, 'B-tim': 0.025, 'I-nat': 0.025, 'B-per': 0.025}, u'NNPS': {'I-art': 0.05, 'B-nat': 0.025, 'B-gpe': 0.025, 'B-art': 0.025, 'I-tim': 0.025, 'B-org': 0.075, 'O': 0.05, 'B-geo': 0.05, 'I-org': 0.325, 'I-geo': 0.2, 'I-per': 0.05, 'I-eve': 0.1, 'B-eve': 0.075, 'I-gpe': 0.125, 'B-tim': 0.025, 'I-nat': 0.025, 'B-per': 0.075}, u'JJS': {'I-art': 0.025, 'B-nat': 0.025, 'B-gpe': 0.025, 'B-art': 0.025, 'I-tim': 0.025, 'B-org': 0.025, 'O': 1.6, 'B-geo': 0.025, 'I-org': 0.025, 'I-geo': 0.025, 'I-per': 0.025, 'I-eve': 0.025, 'B-eve': 0.025, 'I-gpe': 0.025, 'B-tim': 0.025, 'I-nat': 0.025, 'B-per': 0.025}, u'JJR': {'I-art': 0.025, 'B-nat': 0.025, 'B-gpe': 0.025, 'B-art': 0.025, 'I-tim': 0.025, 'B-org': 0.025, 'O': 1.675, 'B-geo': 0.025, 'I-org': 0.025, 'I-geo': 0.025, 'I-per': 0.025, 'I-eve': 0.025, 'B-eve': 0.025, 'I-gpe': 0.025, 'B-tim': 0.025, 'I-nat': 0.025, 'B-per': 0.025}, u'VB': {'I-art': 0.025, 'B-nat': 0.025, 'B-gpe': 0.025, 'B-art': 0.025, 'I-tim': 0.025, 'B-org': 0.025, 'O': 12.075, 'B-geo': 0.025, 'I-org': 0.025, 'I-geo': 0.025, 'I-per': 0.025, 'I-eve': 0.025, 'B-eve': 0.025, 'I-gpe': 0.025, 'B-tim': 0.025, 'I-nat': 0.025, 'B-per': 0.025}}\n"
     ]
    }
   ],
   "source": [
    "# instead, for the purpose of extending the model, make shape_probs and \n",
    "# commit_probs hold dictionaries for the probabilities of each tag\n",
    "\n",
    "\n",
    "# build a dict linking shape to likelihood of each tag\n",
    "\n",
    "shape_probs = {}\n",
    "pos_probs = {}\n",
    "\n",
    "# if X shape, then what is the most likely tag?\n",
    "\n",
    "print(tag_list)\n",
    "print\n",
    "\n",
    "alpha = 1.0\n",
    "\n",
    "for shape in shape_list:\n",
    "    tag_prob_dict = {}\n",
    "    for tag in tag_list:\n",
    "        count = 0\n",
    "        for i in data_small[data_small['shape'] == shape]['tag']:\n",
    "            if i == tag:\n",
    "                count += 1\n",
    "        tag_prob_dict.update({str(tag) : (1.0*count + alpha)/(len(data_small[data_small['shape'] == shape]) + alpha*len(shape_list))})\n",
    "    shape_probs[shape] = tag_prob_dict\n",
    "    \n",
    "for pos in pos_list:\n",
    "    tag_prob_dict = {}\n",
    "    for tag in tag_list:\n",
    "        count = 0\n",
    "        for i in data_small[data_small['pos'] == pos]['tag']:\n",
    "            if i == tag:\n",
    "                count += 1\n",
    "        tag_prob_dict.update({str(tag) : (1.0*count + alpha)/(len(data_small[data_small['shape'] == pos]) + 1.0*len(pos_list))})\n",
    "    pos_probs[pos] = tag_prob_dict\n",
    "\n",
    "print(shape_probs)\n",
    "print\n",
    "print(pos_probs)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'I-art', u'B-gpe', u'B-art', u'I-per', u'I-tim', u'B-org', u'O', u'B-geo', u'B-tim', u'I-geo', u'B-per', u'I-eve', u'B-eve', u'I-gpe', u'I-org', u'I-nat', u'B-nat']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# follow the same procedure for each word. This may take a while to run\n",
    "\n",
    "\n",
    "# build a dict linking shape to likelihood of each tag\n",
    "\n",
    "word_list = list(set(data_small['word']))\n",
    "word_probs = {}\n",
    "alpha = 1.0\n",
    "\n",
    "# if X shape, then what is the most likely tag?\n",
    "\n",
    "print(tag_list)\n",
    "print\n",
    "\n",
    "for word in word_list:\n",
    "    tag_prob_dict = {}\n",
    "    for tag in tag_list:\n",
    "        count = 0\n",
    "        for i in data_small[data_small['word'] == word]['tag']:\n",
    "            if i == tag:\n",
    "                count += 1\n",
    "        tag_prob_dict[tag] = (1.0*count + alpha)/(len(data_small[data_small['shape'] == shape]) + alpha*len(word_list))\n",
    "    word_probs[word] = tag_prob_dict\n",
    "\n",
    "print(word_probs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# now re-write the prediction algorithm to take the max probability for one feature\n",
    "\n",
    "pred_train = []\n",
    "pred_valid = []\n",
    "\n",
    "# make predictions based off of \"shape\"\n",
    "\n",
    "num_O = len(data[data['tag'] == 'O'])\n",
    "percent = 1.0*num_O/len(data)\n",
    "print \"Percent of data without a tag (Baseline accuracy if we only predict 'O': \" + str(percent)\n",
    "\n",
    "# training prediction\n",
    "count_correct = 0\n",
    "for i in range(len(data_small)):\n",
    "    try:\n",
    "        # get the key corresponding to the max value in dict\n",
    "        dict_use = pos_probs[data_small.iloc[i]['pos']]\n",
    "        pred_tag = max(dict_use.iterkeys(), key=(lambda key: dict_use[key]))\n",
    "        # pred_tag = pos_probs[data_small.iloc[i]['pos']]\n",
    "    except:\n",
    "        do='nothing' # figure out this case later\n",
    "        #print(data_small.iloc[i]['pos'])\n",
    "    pred_train.append(pred_tag)\n",
    "    if data_small.iloc[i]['tag'] == pred_tag:\n",
    "        count_correct += 1\n",
    "train_accuracy = 1.0*count_correct / len(data_small)\n",
    "print \"Train Accuracy using pos: \" + str(train_accuracy)\n",
    "\n",
    "# validation prediction\n",
    "count_correct = 0\n",
    "for i in range(len(data_valid)):\n",
    "    try:\n",
    "        # get the key corresponding to the max value in dict\n",
    "        dict_use = pos_probs[data_valid.iloc[i]['pos']]\n",
    "        pred_tag = max(dict_use.iterkeys(), key=(lambda key: dict_use[key]))\n",
    "        #pred_tag = pos_probs[data_valid.iloc[i]['pos']]\n",
    "    except:\n",
    "        do='nothing' # figure out this case later\n",
    "        #print(data_valid.iloc[i]['pos'])\n",
    "    pred_train.append(pred_tag)\n",
    "    if data_valid.iloc[i]['tag'] == pred_tag:\n",
    "        count_correct += 1\n",
    "train_accuracy = 1.0*count_correct / len(data_valid)\n",
    "print \"Validation Accuracy using pos: \" + str(train_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# and try using words\n",
    "\n",
    "# training prediction\n",
    "count_correct = 0\n",
    "for i in range(len(data_small)):\n",
    "    try:\n",
    "        # get the key corresponding to the max value in dict\n",
    "        dict_use = word_probs[data_small.iloc[i]['pos']]\n",
    "        pred_tag = max(dict_use.iterkeys(), key=(lambda key: dict_use[key]))\n",
    "        # pred_tag = pos_probs[data_small.iloc[i]['pos']]\n",
    "    except:\n",
    "        do='nothing' # figure out this case later\n",
    "        #print(data_small.iloc[i]['pos'])\n",
    "    pred_train.append(pred_tag)\n",
    "    if data_small.iloc[i]['tag'] == pred_tag:\n",
    "        count_correct += 1\n",
    "train_accuracy = 1.0*count_correct / len(data_small)\n",
    "print \"Train Accuracy using word: \" + str(train_accuracy)\n",
    "\n",
    "# validation prediction\n",
    "count_correct = 0\n",
    "for i in range(len(data_valid)):\n",
    "    try:\n",
    "        # get the key corresponding to the max value in dict\n",
    "        dict_use = word_probs[data_valid.iloc[i]['pos']]\n",
    "        pred_tag = max(dict_use.iterkeys(), key=(lambda key: dict_use[key]))\n",
    "        #pred_tag = pos_probs[data_valid.iloc[i]['pos']]\n",
    "    except:\n",
    "        do='nothing' # figure out this case later\n",
    "        #print(data_valid.iloc[i]['pos'])\n",
    "    pred_train.append(pred_tag)\n",
    "    if data_valid.iloc[i]['tag'] == pred_tag:\n",
    "        count_correct += 1\n",
    "train_accuracy = 1.0*count_correct / len(data_valid)\n",
    "print \"Validation Accuracy using word: \" + str(train_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred_train = []\n",
    "pred_valid = []\n",
    "\n",
    "# make predictions based off of \"shape\"\n",
    "\n",
    "num_O = len(data[data['tag'] == 'O'])\n",
    "percent = 1.0*num_O/len(data)\n",
    "print \"Percent of data without a tag (Baseline accuracy if we only predict 'O': \" + str(percent)\n",
    "\n",
    "# training prediction\n",
    "count_correct = 0\n",
    "for i in range(len(data_small)):\n",
    "    pred_tag = shape_probs[data_small.iloc[i]['shape']]\n",
    "    pred_train.append(pred_tag)\n",
    "    if data_small.iloc[i]['tag'] == pred_tag:\n",
    "        count_correct += 1\n",
    "train_accuracy = 1.0*count_correct / len(data_small)\n",
    "print \"Train Accuracy using shape: \" + str(train_accuracy)\n",
    "\n",
    "# validation prediction\n",
    "count_correct = 0\n",
    "for i in range(len(data_valid)):\n",
    "    pred_tag = shape_probs[data_valid.iloc[i]['shape']]\n",
    "    pred_train.append(pred_tag)\n",
    "    if data_valid.iloc[i]['tag'] == pred_tag:\n",
    "        count_correct += 1\n",
    "train_accuracy = 1.0*count_correct / len(data_valid)\n",
    "print \"Validation Accuracy using shape: \" + str(train_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# then we can incorporate multiple features in the algorithm by multiply probabilities and taking a max\n",
    "# or adding log probabilities. This is more complicated, but will evertually be the basis of the final model. \n",
    "\n",
    "\n",
    "print(len(data_small))\n",
    "# training prediction\n",
    "count_correct = 0\n",
    "for i in range(len(data_small)):\n",
    "    if i % 100 == 0:\n",
    "        print(i)\n",
    "    tag_probs = {}\n",
    "    for tag in tag_list:\n",
    "        pos_p = pos_probs[data_small.iloc[i]['pos']][tag]\n",
    "        shape_p = shape_probs[data_small.iloc[i]['shape']][tag]\n",
    "        word_p = word_probs[data_small.iloc[i]['word']][tag]\n",
    "        tag_probs[tag] = pos_p * shape_p * word_p \n",
    "    pred_tag = max(tag_probs.iterkeys(), key=(lambda key: dict_use[key]))\n",
    "    pred_train.append(pred_tag)\n",
    "    if data_small.iloc[i]['tag'] == pred_tag:\n",
    "        count_correct += 1\n",
    "train_accuracy = 1.0*count_correct / len(data_small)\n",
    "print \"Train Accuracy using pos, shape, and word: \" + str(train_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# data science work (maybe CV) to tune the model, figure out which features are most predictive\n",
    "# additionally, test out working with other features that we can extract (without breaking independence\n",
    "# assumptions of the HMM)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
